# E-Commerce Data Warehouse

## Overview  
**ecommerce-dwh** is a data warehousing project that consolidates and transforms e-commerce sales data into a structured, analytics-ready format.  
It provides a complete ETL pipeline: data ingestion â†’ cleansing â†’ transformation â†’ dimensional modeling â†’ analytics storage.

## Architecture

The project follows a **star schema** design with:
- **Fact Table**: `fact_sales` (transactional records)
- **Dimension Tables**: `dim_customers`, `dim_products`, `dim_time` (descriptive attributes)
- **Staging Table**: `raw_sales` (raw data before transformation)
- **Transformation Table**: `cleaned_sales` (cleansed data)

## Repository Structure  

```
ecommerce-dwh/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ pipeline.py              # Main ETL pipeline class
â”‚   â””â”€â”€ config.py                # Configuration & environment variables
â”œâ”€â”€ data/
â”‚   â””â”€â”€ online_retail_data.csv   # Raw input data file
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ pricing_anomalies.csv    # Data quality issues detected during load
â”œâ”€â”€ DB_DWH_DDL.sql               # Database schema & table definitions
â”œâ”€â”€ E-Commerce Sales Cleansing.ipynb  # Data exploration & analysis notebook
â”œâ”€â”€ EDA.sql                       # SQL exploratory data analysis queries
â”œâ”€â”€ main.py                       # Entry point to run the ETL pipeline
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env                          # Environment variables (âš ï¸ DO NOT COMMIT)
â”œâ”€â”€ .gitignore                    # Git ignore rules
â””â”€â”€ README.md                     # This file
```

## Prerequisites  

- **Python 3.8+**
- **SQL Server 2016+** (with ODBC Driver 17)
- **ODBC Driver 17 for SQL Server** installed
- Python packages (see `requirements.txt`)

## Installation & Setup

### 1. Clone the repository
```bash
git clone https://github.com/Muhammadibra40/ecommerce-dwh.git
cd ecommerce-dwh
```

### 2. Create and activate virtual environment
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up database
```bash
# Execute the DDL script in SQL Server to create tables & schema
sqlcmd -S <your_server> -d <your_database> -i DB_DWH_DDL.sql
```

### 5. Configure environment variables
Create `.env` file in the root directory:
```env
DB_SERVER=your_sql_server_name
DB_NAME=ECommerceAnalytics
DATA_PATH=data/data.csv
```

 **Security**: Never commit `.env` with real credentials. Add to `.gitignore`.

### 6. Run the ETL pipeline
```bash
python main.py
```

## ETL Pipeline Flow

The pipeline executes the following steps:

1. **Extract** â†’ Read CSV file into pandas DataFrame
2. **Clean** â†’ Remove duplicates, nulls, invalid prices; standardize formats
3. **Load** â†’ Insert raw data into `raw_sales` table with batch processing
4. **Transform** â†’ Execute stored procedure `sp_CleanAndTransferData` to populate `cleaned_sales`
5. **Dimensions** â†’ Populate `dim_customers`, `dim_products`, `dim_time` tables
6. **Facts** â†’ Load `fact_sales` with aggregated metrics

**Output**: Success/failure status + execution time logged to console

## Key Features

âœ… **Data Validation**: Detects pricing anomalies & invalid records (logged to `logs/pricing_anomalies.csv`)  
âœ… **Batch Processing**: Loads data in 1000-row batches for performance  
âœ… **Error Handling**: Comprehensive try-catch with rollback on failures  
âœ… **Dimension Management**: MERGE operations to handle incremental updates  
âœ… **Logging**: Pipeline progress & data quality metrics printed to console  

## Testing & Exploration

### Run exploratory analysis
```bash
# Use Jupyter to explore data before pipeline execution
jupyter notebook E-Commerce Sales Cleansing.ipynb
```

### Validate data warehouse
```bash
# Execute SQL queries to inspect results
sqlcmd -S <your_server> -d <your_database> -i EDA.sql
```

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| "Missing environment variables" | Check `.env` file exists with `DB_SERVER`, `DB_NAME`, `DATA_PATH` |
| "No results. Previous SQL was not a query" | Ensure stored procedure returns result set (check `sp_CleanAndTransferData`) |
| "Connection refused" | Verify SQL Server is running & ODBC Driver 17 is installed |
| Data not inserted | Check `logs/pricing_anomalies.csv` for rejected records |

## Possible Extensions

- âœ¨ Automated scheduling (Task Scheduler / Airflow / Azure Data Factory)
- ğŸ“ˆ BI dashboards (Power BI / Tableau integration)
- ğŸ” Additional data sources (inventory, marketing, customer behavior)
- âœ… Unit tests & CI/CD pipeline
- ğŸ“Š Advanced analytics models (RFM, clustering, forecasting)

## Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Make changes & test end-to-end: `python main.py`
4. Commit with clear messages: `git commit -m "Add feature X"`
5. Submit a pull request

Ensure the pipeline runs successfully before submitting PRs.


## Contact

For questions, please contact me on:

- ğŸ“§ **Email**: [migibra678@gmail.com](mailto:migibra678@gmail.com)
- ğŸ’¼ **LinkedIn**: [Muhammad Ibrahim](https://www.linkedin.com/in/muhammad-ibrahim-093293218/)
